web crawler wikipedia web crawler from wikipedia free encyclopedia jump navig jump search softwar systemat brows world wide web this articl internet bot for search engin see webcrawl web spider redirect it confus spider web spiderbot redirect for video game see arac video game architectur web crawler a web crawler sometim call spider spiderbot often shorten crawler internet bot systemat brows world wide web typic purpos web index web spider web search engin websit use web crawl spider softwar updat web content indic site web content web crawler copi page process search engin index download page user search effici crawler consum resourc visit system often visit site without approv issu schedul load polit come play larg collect page access mechan exist public site wish crawl make known crawl agent for exampl includ robotstxt file request bot index part websit noth the number internet page extrem larg even largest crawler fall short make complet index for reason search engin struggl give relev search result earli year world wide web 2000 today relev result given almost instant crawler valid hyperlink html code they also use web scrape see also datadriven program content 1 nomenclatur 2 overview 3 crawl polici 31 select polici 311 restrict follow link 312 url normal 313 pathascend crawl 314 focus crawl 3141 academicfocus crawler 3142 semant focus crawler 32 revisit polici 33 polit polici 34 parallel polici 4 architectur 5 secur 6 crawler identif 7 crawl deep web 71 web crawler bias 8 visual vs programmat crawler 9 exampl 91 opensourc crawler 10 see also 11 refer 12 further read nomenclatureedit a web crawler also known spider1 ant automat indexer2 foaf softwar context web scutter3 overviewedit a web crawler start list url visit call seed as crawler visit url identifi hyperlink page add list url visit call crawl frontier url frontier recurs visit accord set polici if crawler perform archiv websit web archiv copi save inform goe the archiv usual store way view read navig live web preserv snapshots4 the archiv known repositori design store manag collect web page the repositori store html page page store distinct file a repositori similar system store data like modernday databas the differ repositori need function offer databas system the repositori store recent version web page retriev crawler5 the larg volum impli crawler download limit number web page within given time need priorit download the high rate chang impli page might alreadi updat even delet the number possibl url crawl generat serversid softwar also made difficult web crawler avoid retriev duplic content endless combin http get urlbas paramet exist small select actual return uniqu content for exampl simpl onlin photo galleri may offer three option user specifi http get paramet url if exist four way sort imag three choic thumbnail size two file format option disabl userprovid content set content access 48 differ url may link site this mathemat combin creat problem crawler must sort endless combin relat minor script chang order retriev uniqu content as edward et al note given bandwidth conduct crawl neither infinit free becom essenti crawl web scalabl effici way reason measur qualiti fresh maintained6 a crawler must care choos step page visit next crawl policyedit the behavior web crawler outcom combin policies7 select polici state page download revisit polici state check chang page polit polici state avoid overload web site parallel polici state coordin distribut web crawler select policyedit given current size web even larg search engin cover portion public avail part a 2009 studi show even largescal search engin index 4070 index web8 previous studi steve lawrenc lee gile show search engin index 16 web 19999 as crawler alway download fraction web page high desir download fraction contain relev page random sampl web this requir metric import priorit web page the import page function intrins qualiti popular term link visit even url latter case vertic search engin restrict singl toplevel domain search engin restrict fix web site design good select polici ad difficulti must work partial inform complet set web page known crawl junghoo cho et al made first studi polici crawl schedul their data set 180000page crawl stanfordedu domain crawl simul done differ strategies10 the order metric test breadthfirst backlink count partial pagerank calcul one conclus crawler want download page high pagerank earli crawl process partial pagerank strategi better follow breadthfirst backlinkcount howev result singl domain cho also wrote phd dissert stanford web crawling11 najork wiener perform actual crawl 328 million page use breadthfirst ordering12 they found breadthfirst crawl captur page high pagerank earli crawl compar strategi strategi the explan given author result import page mani link numer host link found earli regardless host page crawl origin abiteboul design crawl strategi base algorithm call opic onlin page import computation13 in opic page given initi sum cash distribut equal among page point it similar pagerank comput faster done one step an opicdriven crawler download first page crawl frontier higher amount cash experi carri 100000page synthet graph powerlaw distribut inlink howev comparison strategi experi real web boldi et al use simul subset web 40 million page domain 100 million page webbas crawl test breadthfirst depthfirst random order omnisci strategi the comparison base well pagerank comput partial crawl approxim true pagerank valu surpris visit accumul pagerank quick notabl breadthfirst omnisci visit provid poor progress approximations1415 baezay et al use simul two subset web 3 million page gr cl domain test sever crawl strategies16 they show opic strategi strategi use length persit queue better breadthfirst crawl also effect use previous crawl avail guid current one daneshpajouh et al design communiti base algorithm discov good seeds17 their method crawl web page high pagerank differ communiti less iter comparison crawl start random seed one extract good seed previouslycrawledweb graph use new method use seed new crawl effect restrict follow linksedit a crawler may want seek html page avoid mime type in order request html resourc crawler may make http head request determin web resourc mime type request entir resourc get request to avoid make numer head request crawler may examin url request resourc url end certain charact html htm asp aspx php jsp jspx slash this strategi may caus numer html web resourc unintent skip some crawler may also avoid request resourc dynam produc order avoid spider trap may caus crawler download infinit number url web site this strategi unreli site use url rewrit simplifi url url normalizationedit main articl url normal crawler usual perform type url normal order avoid crawl resourc the term url normal also call url canonic refer process modifi standard url consist manner there sever type normal may perform includ convers url lowercas remov segment ad trail slash nonempti path component18 pathascend crawlingedit some crawler intend downloadupload mani resourc possibl particular web site so pathascend crawler introduc would ascend everi path url intend crawl19 for exampl given seed url httpllamaorghamstermonkeypagehtml attempt crawl hamstermonkey hamster cothey found pathascend crawler effect find isol resourc resourc inbound link would found regular crawl focus crawlingedit main articl focus crawler the import page crawler also express function similar page given queri web crawler attempt download page similar call focus crawler topic crawler the concept topic focus crawl first introduc filippo menczer2021 soumen chakrabarti et al22 the main problem focus crawl context web crawler would like abl predict similar text given page queri actual download page a possibl predictor anchor text link approach taken pinkerton23 first web crawler earli day web diligenti et al24 propos use complet content page alreadi visit infer similar drive queri page visit yet the perform focus crawl depend most rich link specif topic search focus crawl usual reli general web search engin provid start point academicfocus crawleredit an exampl focus crawler academ crawler crawl freeaccess academ relat document citeseerxbot crawler citeseerx search engin other academ search engin googl scholar microsoft academ search etc becaus academ paper publish pdf format kind crawler particular interest crawl pdf postscript file microsoft word includ zip format becaus general open sourc crawler heritrix must custom filter mime type middlewar use extract document import focus crawl databas repository25 identifi whether document academ challeng add signific overhead crawl process perform post crawl process use machin learn regular express algorithm these academ document usual obtain home page faculti student public page research institut becaus academ document take small fraction entir web page good seed select import boost effici web crawlers26 other academ crawler may download plain text html file contain metadata academ paper titl paper abstract this increas overal number paper signific fraction may provid free pdf download semant focus crawleredit anoth type focus crawler semant focus crawler make use domain ontolog repres topic map link web page relev ontolog concept select categor purposes27 in addit ontolog automat updat crawl process dong et al28 introduc ontologylearningbas crawler use support vector machin updat content ontolog concept crawl web page revisit policyedit the web dynam natur crawl fraction web take week month by time web crawler finish crawl mani event could happen includ creation updat delet from search engin point view cost associ detect event thus outdat copi resourc the mostus cost function fresh age29 fresh this binari measur indic whether local copi accur the fresh page p repositori time defin f p 1 f p e q u l h e l c l c p e 0 h e r w e displaystyl fpt begincases1rm if p rm is equal to the local copi at time t0rm otherwiseendcas age this measur indic outdat local copi the age page p repositori time defin a p 0 f p n f e e f c n e f p h e r w e displaystyl apt begincases0rm if p rm is not modifi at time ttrm modif time of prm otherwiseendcas coffman et al work definit object web crawler equival fresh use differ word propos crawler must minim fraction time page remain outdat they also note problem web crawl model multiplequeu singleserv poll system web crawler server web site queue page modif arriv custom switchov time interv page access singl web site under model mean wait time custom poll system equival averag age web crawler30 the object crawler keep averag fresh page collect high possibl keep averag age page low possibl these object equival first case crawler concern mani page outdat second case crawler concern old local copi page evolut fresh age web crawler two simpl revisit polici studi cho garciamolina31 uniform polici this involv revisit page collect frequenc regardless rate chang proport polici this involv revisit often page chang frequent the visit frequenc direct proport estim chang frequenc in case repeat crawl order page done either random fix order cho garciamolina prove surpris result term averag fresh uniform polici outperform proport polici simul web real web crawl intuit reason web crawler limit mani page crawl given time frame 1 alloc mani new crawl rapid chang page expens less frequent updat page 2 fresh rapid chang page last shorter period less frequent chang page in word proport polici alloc resourc crawl frequent updat page experi less overal fresh time to improv fresh crawler penal element chang often32 the optim revisit polici neither uniform polici proport polici the optim method keep averag fresh high includ ignor page chang often optim keep averag age low use access frequenc monoton sublinear increas rate chang page in case optim closer uniform polici proport polici coffman et al note order minim expect obsolesc time access particular page kept even space possible30 explicit formula revisit polici attain general obtain numer depend distribut page chang cho garciamolina show exponenti distribut good fit describ page changes32 ipeiroti et al show use statist tool discov paramet affect distribution33 note revisit polici consid regard page homogen term qualiti page web worth someth realist scenario inform web page qualiti includ achiev better crawl polici polit policyedit crawler retriev data much quicker greater depth human searcher crippl impact perform site if singl crawler perform multipl request per second andor download larg file server hard time keep request multipl crawler as note koster use web crawler use number task come price general community34 the cost use web crawler includ network resourc crawler requir consider bandwidth oper high degre parallel long period time server overload especi frequenc access given server high poor written crawler crash server router download page cannot handl person crawler deploy mani user disrupt network web server a partial solut problem robot exclus protocol also known robotstxt protocol standard administr indic part web server access crawlers35 this standard includ suggest interv visit server even though interv effect way avoid server overload recent commerci search engin like googl ask jeev msn yahoo search abl use extra crawldelay paramet robotstxt file indic number second delay request the first propos interv success pageload 60 seconds36 howev page download rate websit 100000 page perfect connect zero latenc infinit bandwidth would take 2 month download entir web site also fraction resourc web server would use this seem accept cho use 10 second interv accesses31 wire crawler use 15 second default37 the mercatorweb crawler follow adapt polit polici took second download document given server crawler wait 10t second download next page38 dill et al use 1 second39 for use web crawler research purpos detail costbenefit analysi need ethic consider taken account decid crawl fast crawl40 anecdot evid access log show access interv known crawler vari 20 second 34 minut it worth notic even polit take safeguard avoid overload web server complaint web server administr receiv brin page note run crawler connect half million server generat fair amount email phone call becaus vast number peopl come line alway know crawler first one seen41 parallel policyedit main articl distribut web crawl a parallel crawler crawler run multipl process parallel the goal maxim download rate minim overhead parallel avoid repeat download page to avoid download page crawl system requir polici assign new url discov crawl process url found two differ crawl process architecturesedit highlevel architectur standard web crawler a crawler must good crawl strategi note previous section also high optim architectur shkapenyuk suel note that42 while fair easi build slow crawler download page per second short period time build highperform system download hundr million page sever week present number challeng system design io network effici robust manag web crawler central part search engin detail algorithm architectur kept busi secret when crawler design publish often import lack detail prevent other reproduc work there also emerg concern search engin spam prevent major search engin publish rank algorithm securityedit while websit owner keen page index broad possibl strong presenc search engin web crawl also unintend consequ lead compromis data breach search engin index resourc shouldnt public avail page reveal potenti vulner version softwar main articl googl hack apart standard web applic secur recommend websit owner reduc exposur opportunist hack allow search engin index public part websit robotstxt explicit block index transact part login page privat page etc crawler identificationedit web crawler typic identifi web server use userag field http request web site administr typic examin web server log use user agent field determin crawler visit web server often the user agent field may includ url web site administr may find inform crawler examin web server log tedious task therefor administr use tool identifi track verifi web crawler spambot malici web crawler unlik place identifi inform user agent field may mask ident browser wellknown crawler it import web crawler identifi web site administr contact owner need in case crawler may accident trap crawler trap may overload web server request owner need stop crawler identif also use administr interest know may expect web page index particular search engin crawl deep webedit a vast amount web page lie deep invis web43 these page typic access submit queri databas regular crawler unabl find page link point googl sitemap protocol mod oai44 intend allow discoveri deepweb resourc deep web crawl also multipli number web link crawl some crawler take url a href url form in case googlebot web crawl done text contain insid hypertext content tag text strateg approach may taken target deep web content with techniqu call screen scrape special softwar may custom automat repeat queri given web form intent aggreg result data such softwar use span multipl web form across multipl websit data extract result one web form submiss taken appli input anoth web form thus establish continu across deep web way possibl tradit web crawlers45 page built ajax among caus problem web crawler googl propos format ajax call bot recogn index46 web crawler biasedit a recent studi base larg scale analysi robotstxt file show certain web crawler prefer other googlebot prefer web crawler47 visual vs programmat crawlersedit there number visual web scrapercrawl product avail web crawl page structur data column row base user requir one main differ classic visual crawler level program abil requir set crawler the latest generat visual scraper remov major program skill need abl program start crawl scrape web data the visual scrapingcrawl method reli user teach piec crawler technolog follow pattern semistructur data sourc the domin method teach visual crawler highlight data browser train column row while technolog new exampl basi needlebas bought googl part larger acquisit ita labs48 continu growth invest area investor endusers49 examplesedit this articl may contain indiscrimin excess irrelev exampl pleas improv articl ad descript text remov less pertin exampl see wikipedia guid write better articl suggest may 2012 the follow list publish crawler architectur generalpurpos crawler exclud focus web crawler brief descript includ name given differ compon outstand featur bingbot name microsoft bing webcrawl it replac msnbot baiduspid baidus web crawler googlebot describ detail refer earli version architectur written c python the crawler integr index process text pars done fulltext index also url extract there url server send list url fetch sever crawl process dure pars url found pass url server check url previous seen if url ad queue url server sortsit swiftbot swiftyp web crawler webcrawl use build first public avail fulltext index subset web it base libwww download page anoth program pars order url breadthfirst explor web graph it also includ realtim crawler follow link base similar anchor text provid queri webfountain distribut modular crawler similar mercat written c world wide web worm crawler use build simpl index document titl url the index could search use grep unix command xenon web crawler use govern tax author detect fraud5051 yahoo slurp name yahoo search crawler yahoo contract microsoft use bingbot instead opensourc crawlersedit frontera web crawl framework implement crawl frontier compon provid scalabl primit web crawler applic gnu wget commandlineoper crawler written c releas gpl it typic use mirror web ftp site grub open sourc distribut search crawler wikia search use crawl web heritrix internet archiv archivalqu crawler design archiv period snapshot larg portion web it written java htdig includ web crawler index engin httrack use web crawler creat mirror web site offlin view it written c releas gpl mnogosearch crawler index search engin written c licens gpl nix machin norconex http collector web spider crawler written java aim make enterpris search integr develop life easier licens apach licens apach nutch high extens scalabl web crawler written java releas apach licens it base apach hadoop use apach solr elasticsearch open search server search engin web crawler softwar releas gpl phpcrawler simpl php mysql base crawler releas bsd licens scrapi open sourc webcrawl framework written python licens bsd seek free distribut search engin licens agpl stormcrawl collect resourc build lowlat scalabl web crawler apach storm apach licens tkwww robot crawler base tkwww web browser licens gpl xapian search crawler engin written c yaci free distribut search engin built principl peertop network licens gpl trandoshan free open sourc distribut webcrawl design deepweb see alsoedit automat index gnutella crawler web archiv webgraph websit mirror softwar search engin scrape web scrape referencesedit spetka scott the tkwww robot beyond brows ncsa archiv origin 3 septemb 2004 retriev 21 novemb 2010 kobayashi m takeda k 2000 inform retriev web acm comput survey 32 2 144173 citeseerx 10111266094 doi101145358923358934 s2cid 3710903 see definit scutter foaf project wiki masanè julien 15 februari 2007 web archiv springer p 1 isbn 9783540463320 retriev 24 april 2014 patil yugandhara patil sonal 2016 review web crawler specif work pdf intern journal advanc research comput communic engin 5 1 4 edward j mccurley k s tomlin j a 2001 an adapt model optim perform increment web crawler proceed tenth intern confer world wide web www 01 in proceed tenth confer world wide web pp 106113 citeseerx 101110181506 doi101145371920371960 isbn 9781581133486 s2cid 10316730cs1 maint multipl name author list link castillo carlo 2004 effect web crawl phd thesi univers chile retriev 3 august 2010 a gull a signori 2005 the index web 115 billion page special interest track poster 14th intern confer world wide web acm press pp 902903 doi10114510627451062789 steve lawrenc c lee gile 8 juli 1999 access inform web natur 400 6740 1079 bibcode1999natur400107l doi10103821987 pmid 10428673 s2cid 4347646 cho j garciamolina h page l april 1998 effici crawl through url order seventh intern worldwid web confer brisban australia doi1011423725 isbn 9789810234003 retriev 23 march 2009 cho junghoo crawl web discoveri mainten largescal web data phd dissert depart comput scienc stanford univers novemb 2001 marc najork janet l wiener breadthfirst crawl yield highqual page in proceed tenth confer world wide web page 114118 hong kong may 2001 elsevi scienc serg abiteboul mihai preda gregori cobena 2003 adapt onlin page import comput proceed 12th intern confer world wide web budapest hungari acm pp 280290 doi101145775152775192 isbn 1581136803 retriev 22 march 2009 paolo boldi bruno codenotti massimo santini sebastiano vigna 2004 ubicrawl scalabl fulli distribut web crawler pdf softwar practic experi 34 8 711726 citeseerx 101125538 doi101002spe587 retriev 23 march 2009 paolo boldi massimo santini sebastiano vigna 2004 do your worst make best paradox effect pagerank increment comput pdf algorithm model webgraph lectur note comput scienc 3243 pp 168180 doi101007978354030216214 isbn 9783540234272 retriev 23 march 2009 baezay r castillo c marin m rodriguez a 2005 crawl countri better strategi breadthfirst web page order in proceed industri practic experi track 14th confer world wide web page 864872 chiba japan acm press shervin daneshpajouh mojtaba mohammadi nasiri mohammad ghodsi a fast communiti base algorithm generat crawler seed set in proceed 4th intern confer web inform system technolog webist2008 funchal portug may 2008 pant gautam srinivasan padmini menczer filippo 2004 crawl web pdf in leven mark poulovassili alexandra ed web dynam adapt chang content size topolog use springer pp 153178 isbn 9783540406761 cothey viv 2004 webcrawl reliabl pdf journal american societi inform scienc technolog 55 14 12281238 citeseerx 1011117185 doi101002asi20078 menczer f 1997 arachnid adapt retriev agent choos heurist neighborhood inform discoveri in d fisher ed machin learn proceed 14th intern confer icml97 morgan kaufmann menczer f belew rk 1998 adapt inform agent distribut textual environ in k sycara m wooldridg ed proc 2nd intl conf autonom agent agent 98 acm press chakrabarti soumen van den berg martin dom byron 1999 focus crawl a new approach topicspecif web resourc discoveri pdf comput network 31 1116 16231640 doi101016s1389128699000523 archiv origin pdf 17 march 2004 pinkerton b 1994 find peopl want experi webcrawl in proceed first world wide web confer geneva switzerland diligenti m coetze f lawrenc s gile c l gori m 2000 focus crawl use context graph in proceed 26th intern confer veri larg databas vldb page 527534 cairo egypt wu jian teregowda pradeep khabsa madian carman stephen jordan dougla san pedro wandelm jose lu xin mitra prasenjit gile c lee 2012 web crawler middlewar search engin digit librari proceed twelfth intern workshop web inform data manag widm 12 p 57 doi10114523899362389949 isbn 9781450317207 s2cid 18513666 wu jian teregowda pradeep ramírez juan pablo fernández mitra prasenjit zheng shuyi gile c lee 2012 the evolut crawl strategi academ document search engin proceed 3rd annual acm web scienc confer web sci 12 pp 340343 doi10114523807182380762 isbn 9781450312288 s2cid 16718130 dong hai hussain farookh khadeer chang elizabeth 2009 state art semant focus crawler comput scienc it applic iccsa 2009 lectur note comput scienc 5593 pp 910924 doi101007978364202457374 hdl205001193748288 isbn 9783642024566 dong hai hussain farookh khadeer 2013 sof a semisupervis ontologylearningbas focus crawler concurr comput practic experi 25 12 17551770 doi101002cpe2980 s2cid 205690364 junghoo cho hector garciamolina 2000 synchron databas improv fresh pdf proceed 2000 acm sigmod intern confer manag data dalla texa unit state acm pp 117128 doi101145342009335391 isbn 1581132174 retriev 23 march 2009 b e g coffman jr zhen liu richard r weber 1998 optim robot schedul web search engin journal schedul 1 1 1529 citeseerx 1011366087 doi101002sici1099142519980611 15aidjos3 30co2k b cho junghoo garciamolina hector 2003 effect page refresh polici web crawler acm transact databas system 28 4 390426 doi101145958942958945 s2cid 147958 b junghoo cho hector garciamolina 2003 estim frequenc chang acm transact internet technolog 3 3 256290 citeseerx 1011595877 doi101145857166857170 s2cid 9362566 ipeiroti p ntoula a cho j gravano l 2005 model manag content chang text databas in proceed 21st ieee intern confer data engin page 606617 april 2005 tokyo koster m 1995 robot web threat treat connexion 94 koster m 1996 a standard robot exclus koster m 1993 guidelin robot writer baezay r castillo c 2002 balanc volum qualiti fresh web crawl in soft comput system design manag applic page 565572 santiago chile io press amsterdam heydon allan najork marc 26 june 1999 mercat a scalabl extens web crawler pdf archiv origin pdf 19 februari 2006 retriev 22 march 2009 cite journal requir journal help dill s kumar r mccurley k s rajagopalan s sivakumar d tomkin a 2002 selfsimilar web pdf acm transact internet technolog 2 3 205223 doi101145572326572328 s2cid 6416041 m thelwal d stuart 2006 web crawl ethic revisit cost privaci denial servic journal american societi inform scienc technolog 57 13 17711779 doi101002asi20388 brin sergey page lawrenc 1998 the anatomi largescal hypertextu web search engin comput network isdn system 30 17 107117 doi101016s016975529800110x shkapenyuk v suel t 2002 design implement high perform distribut web crawler in proceed 18th intern confer data engin icd page 357368 san jose california ieee cs press shestakov deni 2008 search interfac web queri character tuc doctor dissert 104 univers turku michael l nelson herbert van de sompel xiaom liu terri l harrison nathan mcfarland 24 march 2005 modoai an apach modul metadata harvest cs0503069 arxivcs0503069 bibcode2005cs3069n cite journal requir journal help shestakov deni bhowmick sourav s lim eepeng 2005 dequ queri deep web pdf data knowledg engin 52 3 273311 doi101016s0169023x04001077 ajax crawl guid webmast develop retriev 17 march 2013 sun yang 25 august 2008 a comprehens studi of the regul and behavior of web crawler the crawler web spider softwar robot handl trace file brows hundr billion page found web usual determin track keyword make search search engin user factor vari second second accord moz 30 search perform search engin like googl bing yahoo correspond generic word phrase the remain 70 usual random retriev 11 august 2014 cite journal requir journal help ita lab ita lab acquisit 20 april 2011 128 am crunchbasecom march 2014 crunch base profil importio norton quinn 25 januari 2007 tax taker send spider busi wire archiv origin 22 decemb 2016 retriev 13 octob 2017 xenon web crawl initi privaci impact assess pia summari ottawa govern canada 11 april 2017 archiv origin 25 septemb 2017 retriev 13 octob 2017 further readingedit cho junghoo web crawl project ucla comput scienc depart a histori search engin wiley wivet benchmark project owasp aim measur web crawler identifi hyperlink target websit shestakov deni current challeng web crawl intellig web crawl slide tutori given icwe13 wiiat13 v e internet search type web search engin list metasearch engin multimedia search collabor search engin crosslanguag search local search vertic search social search imag search audio search video search engin enterpris search semant search natur languag search engin voic search tool search engin market search engin optim evalu measur search orient architectur selectionbas search document retriev text mine web crawler multisearch feder search search aggreg indexweb index focus crawler spider trap robot exclus standard distribut web crawl web archiv websit mirror softwar web search queri web queri classif protocol standard z3950 searchretriev web servic searchretriev via url opensearch represent state transfer websit pars templat wide area inform server see also search engin desktop search onlin search v e web crawler internet bot design web crawl web index activ 80leg bingbot fetcher googlebot heritrix httrack phpcrawler powermapp wget discontinu fast crawler msnbot rbse tkwww robot twicel type distribut web crawler focus crawler author control gnd 47962987 retriev httpsenwikipediaorgwindexphptitl webcrawleroldid 990832154 categori search engin softwar web crawler internet search algorithm hidden categori cs1 maint multipl name author list cs1 error miss period articl short descript short descript differ wikidata use dmi date septemb 2020 articl mani exampl may 2012 all articl mani exampl wikipedia articl style issu may 2012 wikipedia articl gnd identifi navig menu person tool not log talk contribut creat account log namespac articl talk variant view read edit view histori more search navig main page content current event random articl about wikipedia contact us donat contribut help learn edit communiti portal recent chang upload file tool what link relat chang upload file special page perman link page inform cite page wikidata item printexport download pdf printabl version languag afrikaan azrbaycanca boarisch català eština cymraeg deutsch español euskara françai hrvatski italiano lietuvi magyar bahasa melayu nederland nedersaksi norsk bokmål norsk nynorsk polski portuguê român simpl english srpski suomi svenska türkçe edit link this page last edit 26 novemb 2020 1919 utc text avail creativ common attributionsharealik licens addit term may appli by use site agre term use privaci polici wikipedia regist trademark wikimedia foundat inc nonprofit organ privaci polici about wikipedia disclaim contact wikipedia mobil view develop statist cooki statement
